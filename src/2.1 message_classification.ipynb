{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVXhHL7Hf64J"
   },
   "source": [
    "### Цель:\n",
    "разработать систему, которая находит в предложении название фильма. \n",
    "Если название найдено, формируется итоговое предложение по шаблону. \n",
    "В остальных случаях итоговое предложение не отличается от полученного на вход.\n",
    "\n",
    "### Входные данные:\n",
    "`предложение` с вопросом о фильме & необработанный `список` из доступных названий фильмов\n",
    "### 2 варианта Выходных данных:\n",
    "1. Последовательность вида \"Рецензия на фильм название_фильма:\", \n",
    "если в предложении обнаружено название фильма\n",
    "2. Оригинальное предложение, если название фильма не распознано\n",
    "\n",
    "#### `пример 1:`\n",
    "* `Вход:` \"Как тебе фильм Матрица?\"\n",
    "* `Выход:` \"Рецензия на фильм Матрица:\"\n",
    "\n",
    "#### `пример 2:`\n",
    "* `Вход:` \"Тебе понравилась игра Киану Ривза?\"\n",
    "* `Выход:` \"Тебе понравилась игра Киану Ривза?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQr5vPS-inAw"
   },
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-xYobTGilTs"
   },
   "source": [
    "### Возможный алгоритм\n",
    "\n",
    "1. Создаем словарь из токенизированных названий фильмов, предварительно очистив названия от символьного мусора \"/ : ; ,\" и тд.\n",
    "#### `Пример:` {'Остров проклятых': ['остров','проклятых'], 'Матрица: Воскрешение': ['матрица','воскрешение']}\n",
    "\n",
    "2. Создаем копию предложения и все преобразования делаем на копии\n",
    "3. Разбиваем полученное предложение на токены\n",
    "4. Применяем lowercase для всех букв\n",
    "5. Проводим лемматизацию каждого токена\n",
    "6. Обрезаем каждый токен до 2/3 от его изначальной длины \n",
    "#### `Пример:` 'собрание' -> 'собра', т.к. 8 * 2/3 = 5\n",
    "\n",
    "7. Проводим сравнение последовательностей из предложения со словарем названий.\n",
    "В токенизированном тексте последовательно проверяется каждый токен. \n",
    "\n",
    "(а) если последовательность из токенов предложения совпадает с 1 из последовательностью названия фильма\n",
    "\n",
    "-> в предложении упоминается название фильма \n",
    "\n",
    "-> на выходе система возвращает шаблон \"Рецензия на фильм Оригинальное название фильма:\"\n",
    "#### `Пример:`\n",
    "* ориг. предложение: 'Поделись мнением о фильме Матрица: Воскрешение'\n",
    "* токениз. предложение: ['поде', 'мнен', 'о', 'филь', 'матр', 'воскреш']\n",
    "* словарь названий: {..., 'Остров проклятых': ['остров','проклятых'], 'Матрица: Воскрешение': ['матрица','воскрешение'], ...}\n",
    "\n",
    "\n",
    "* выход: 'Рецензия на фильм Матрица: Воскрешение:' \n",
    "(т.к. последовательность 'матр', 'воскреш' входит в последовательность 'матрица','воскрешение')\n",
    "\n",
    "(б) если ни одно совпадение не обнаружено \n",
    "-> на выходе система возвращает оригинальное предложение\n",
    "#### `Пример:`\n",
    "* ориг. предложение: 'Почему Николас Кейдж снимается в откровенном треше?'\n",
    "\n",
    "\n",
    "* выход: 'Почему Николас Кейдж снимается в откровенном треше?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy pandas\n",
    "#!pip install nltk sklearn  scipy\n",
    "#!pip3 install pymystem3\n",
    "#!pip3 install git+https://github.com/Koziev/rutokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import rutokenizer\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filtered_film_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Помни</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Форрест Гамп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Зеленая миля</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Крестный отец</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Король Лев</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171</td>\n",
       "      <td>Мистер Сталь</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>Побег невозможен</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>173</td>\n",
       "      <td>Милашка в розовом</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>174</td>\n",
       "      <td>Призрак и миссис Мьюр</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>175</td>\n",
       "      <td>Это старое чувство</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    filtered_film_names\n",
       "0             0                  Помни\n",
       "1             1           Форрест Гамп\n",
       "2             2           Зеленая миля\n",
       "3             3          Крестный отец\n",
       "4             4             Король Лев\n",
       "..          ...                    ...\n",
       "171         171           Мистер Сталь\n",
       "172         172       Побег невозможен\n",
       "173         173      Милашка в розовом\n",
       "174         174  Призрак и миссис Мьюр\n",
       "175         175     Это старое чувство\n",
       "\n",
       "[176 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "films = pd.read_csv(\"filtered_films.csv\")\n",
    "films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12 обезьян'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#название фильмы в ловер, лемма, токеннизация \n",
    "test_films = films.copy()\n",
    "test_films = test_films.drop(columns = ['Unnamed: 0'], axis = 1)\n",
    "test_films = test_films['filtered_film_names'].str.lower()\n",
    "\n",
    "test_films[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_cleaning(test_films):\n",
    "    temp = 0\n",
    "    for row in test_films.index:\n",
    "        temp = re.sub(r\"[0-9]{1,}-[а-я]{1,}\", \",\", test_films[row])\n",
    "        temp = re.sub(r\"  \", \"\", temp)\n",
    "        temp = re.sub(r\"[-—.?:;!»«/\\)(#$%^&*№%']\", \" \", temp)\n",
    "        temp = re.sub(r\"[ ]{1,}\", \",\", temp)\n",
    "        if temp.endswith(\" \"):\n",
    "            temp = temp[:-1]\n",
    "        if temp.startswith(\" \"):\n",
    "            temp = temp[1:]\n",
    "        test_films[row] = temp\n",
    "    return test_films\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      помни\n",
       "1               форрест,гамп\n",
       "2               зеленая,миля\n",
       "3              крестный,отец\n",
       "4                 король,лев\n",
       "               ...          \n",
       "171             мистер,сталь\n",
       "172         побег,невозможен\n",
       "173        милашка,в,розовом\n",
       "174    призрак,и,миссис,мьюр\n",
       "175       это,старое,чувство\n",
       "Name: filtered_film_names, Length: 176, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_cleaning(test_films)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12,обезьян'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_films[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'звёздные,войны,эпизод,6,возвращение,джедая'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_films[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "film_tokens = test_films.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(text):\n",
    "    regular = r'[\\*+\\#+\\№\\\"\\-+\\+\\=+\\?+\\&\\^\\.+\\;\\,+\\>+\\(\\)\\/+\\:\\\\+]'\n",
    "    text = re.sub(regular, ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "  \n",
    "cleaned_text = []\n",
    "tokens = []\n",
    " \n",
    "for text in test_films:\n",
    "    text = clean_tokens(text)\n",
    "    cleaned_text.append(text)\n",
    "\n",
    "    text = word_tokenize(text)\n",
    "    tokens.append(text)\n",
    "    \n",
    "film_tokens['cleaned_films'] = cleaned_text\n",
    "film_tokens['tokens_films'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['помни'],\n",
       " ['форрест', 'гамп'],\n",
       " ['зеленая', 'миля'],\n",
       " ['крестный', 'отец'],\n",
       " ['король', 'лев'],\n",
       " ['американская', 'история', 'x'],\n",
       " ['криминальное', 'чтиво'],\n",
       " ['гладиатор'],\n",
       " ['искусственный', 'разум'],\n",
       " ['дракула'],\n",
       " ['анастасия'],\n",
       " ['гражданин', 'кейн'],\n",
       " ['мертвец'],\n",
       " ['привидение'],\n",
       " ['мосты', 'округа', 'мэдисон'],\n",
       " ['большой', 'лебовски'],\n",
       " ['грязные', 'танцы'],\n",
       " ['звонок'],\n",
       " ['амадей'],\n",
       " ['казино'],\n",
       " ['12', 'обезьян'],\n",
       " ['скала'],\n",
       " ['магнолия'],\n",
       " ['чужой', '3'],\n",
       " ['50', 'первых', 'поцелуев'],\n",
       " ['огни', 'большого', 'города'],\n",
       " ['битлджюс'],\n",
       " ['фарго'],\n",
       " ['кейт', 'и', 'лео'],\n",
       " ['простая', 'история'],\n",
       " ['звёздные', 'войны', 'эпизод', '6', 'возвращение', 'джедая'],\n",
       " ['рекрут'],\n",
       " ['смерть', 'ей', 'к', 'лицу'],\n",
       " ['вспомнить', 'всё'],\n",
       " ['убить', 'пересмешника'],\n",
       " ['аладдин'],\n",
       " ['история', 'рыцаря'],\n",
       " ['чего', 'хотят', 'женщины'],\n",
       " ['королевская', 'битва'],\n",
       " ['мумия', 'возвращается'],\n",
       " ['трудная', 'мишень'],\n",
       " ['тельма', 'и', 'луиза'],\n",
       " ['послание', 'в', 'бутылке'],\n",
       " ['бездна'],\n",
       " ['лжец', 'лжец'],\n",
       " ['братство', 'волка'],\n",
       " ['знакомство', 'с', 'родителями'],\n",
       " ['быстрый', 'и', 'мертвый'],\n",
       " ['геркулес'],\n",
       " ['горбун', 'из', 'нотр', 'дама'],\n",
       " ['ганди'],\n",
       " ['энни', 'холл'],\n",
       " ['суперзвезда'],\n",
       " ['крик', '3'],\n",
       " ['мэй'],\n",
       " ['несколько', 'хороших', 'парней'],\n",
       " ['оборотень'],\n",
       " ['каждое', 'воскресенье'],\n",
       " ['взрыв', 'из', 'прошлого'],\n",
       " ['школа', 'рока'],\n",
       " ['траффик'],\n",
       " ['такси', '2'],\n",
       " ['охотники', 'за', 'привидениями'],\n",
       " ['дети', 'шпионов'],\n",
       " ['елена', 'в', 'ящике'],\n",
       " ['неспящие', 'в', 'сиэттле'],\n",
       " ['золотая', 'лихорадка'],\n",
       " ['поцелуй', 'дракона'],\n",
       " ['умри', 'но', 'не', 'сейчас'],\n",
       " ['странные', 'дни'],\n",
       " ['дорога', 'на', 'арлингтон'],\n",
       " ['слезы', 'солнца'],\n",
       " ['ведьмина', 'служба', 'доставки'],\n",
       " ['целуя', 'девушек'],\n",
       " ['дом', 'на', 'краю', 'света'],\n",
       " ['двойная', 'страховка'],\n",
       " ['из', 'африки'],\n",
       " ['джон', 'ф', 'кеннеди', 'выстрелы', 'в', 'далласе'],\n",
       " ['детский', 'час'],\n",
       " ['выбор', 'софи'],\n",
       " ['полицейский', 'из', 'беверли', 'хиллз'],\n",
       " ['французский', 'связной'],\n",
       " ['враг', 'мой'],\n",
       " ['волшебник', 'страны', 'оз'],\n",
       " ['охотники', 'за', 'привидениями', '2'],\n",
       " ['анализируй', 'это'],\n",
       " ['граф', 'монте', 'кристо'],\n",
       " ['бриолин'],\n",
       " ['дорога', 'на', 'эльдорадо'],\n",
       " ['легенда', 'багера', 'ванса'],\n",
       " ['дорожное', 'приключение'],\n",
       " ['дом', 'духов'],\n",
       " ['горячие', 'головы'],\n",
       " ['как', 'важно', 'быть', 'серьезным'],\n",
       " ['комната', 'марвина'],\n",
       " ['кошачий', 'глаз'],\n",
       " ['рок', 'звезда'],\n",
       " ['ядовитый', 'плющ'],\n",
       " ['динозавр'],\n",
       " ['тыквоголовый'],\n",
       " ['нецелованная'],\n",
       " ['шанхайские', 'рыцари'],\n",
       " ['изгои'],\n",
       " ['песни', 'со', 'второго', 'этажа'],\n",
       " ['бунтарь', 'без', 'причины'],\n",
       " ['почтальон', 'всегда', 'звонит', 'дважды'],\n",
       " ['модная', 'мамочка'],\n",
       " ['1492', 'завоевание', 'рая'],\n",
       " ['дик', 'трэйси'],\n",
       " ['нигде'],\n",
       " ['убрать', 'перископ'],\n",
       " ['48', 'часов'],\n",
       " ['проклятие', 'нефритового', 'скорпиона'],\n",
       " ['сильная', 'женщина'],\n",
       " ['в', 'порту'],\n",
       " ['вестсайдская', 'история'],\n",
       " ['телохранитель'],\n",
       " ['моя', 'большая', 'греческая', 'свадьба'],\n",
       " ['язык', 'нежности'],\n",
       " ['шестнадцать', 'свечей'],\n",
       " ['красавчик', 'джонни'],\n",
       " ['клуб', 'первых', 'жен'],\n",
       " ['рождение', 'нации'],\n",
       " ['другие', 'ипостаси'],\n",
       " ['разбирая', 'гарри'],\n",
       " ['101', 'далматинец'],\n",
       " ['пули', 'над', 'бродвеем'],\n",
       " ['гроздья', 'гнева'],\n",
       " ['деловая', 'женщина'],\n",
       " ['отточенное', 'лезвие'],\n",
       " ['незабываемый', 'роман'],\n",
       " ['филадельфийская', 'история'],\n",
       " ['его', 'девушка', 'пятница'],\n",
       " ['конвой'],\n",
       " ['тумстоун', 'легенда', 'дикого', 'запада'],\n",
       " ['ценности', 'семейки', 'аддамс'],\n",
       " ['бесшабашное', 'ограбление'],\n",
       " ['робин', 'гуд', 'мужчины', 'в', 'трико'],\n",
       " ['пришельцы', 'в', 'америке'],\n",
       " ['леди', 'джейн'],\n",
       " ['графиня', 'из', 'гонконга'],\n",
       " ['два', 'мула', 'для', 'сестры', 'сары'],\n",
       " ['зазубренное', 'лезвие'],\n",
       " ['до', 'и', 'после'],\n",
       " ['магазинчик', 'ужасов'],\n",
       " ['голый', 'пистолет', '2', '1', '2', 'запах', 'страха'],\n",
       " ['всемирная', 'история', 'часть', '1'],\n",
       " ['джулия'],\n",
       " ['ночь', 'в', 'баре', 'маккула'],\n",
       " ['убийства', 'в', 'черри', 'фолс'],\n",
       " ['полтергейст', '3'],\n",
       " ['люди', 'кошки'],\n",
       " ['герой'],\n",
       " ['гамбит'],\n",
       " ['вокруг', 'света', 'за', '80', 'дней'],\n",
       " ['нортфорк'],\n",
       " ['жена', 'епископа'],\n",
       " ['пошел', 'ты', 'фредди'],\n",
       " ['увлечение'],\n",
       " ['человек', 'с', 'рентгеновскими', 'глазами'],\n",
       " ['придурок'],\n",
       " ['анализируй', 'то'],\n",
       " ['вход', 'и', 'выход'],\n",
       " ['теленовости'],\n",
       " ['тот', 'кто', 'меня', 'бережет'],\n",
       " ['девять', 'месяцев'],\n",
       " ['молодая', 'кровь'],\n",
       " ['арарат'],\n",
       " ['к2', 'предельная', 'высота'],\n",
       " ['сейчас', 'и', 'тогда'],\n",
       " ['защищая', 'твою', 'жизнь'],\n",
       " ['мистер', 'сталь'],\n",
       " ['побег', 'невозможен'],\n",
       " ['милашка', 'в', 'розовом'],\n",
       " ['призрак', 'и', 'миссис', 'мьюр'],\n",
       " ['это', 'старое', 'чувство']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "def lemmatize_str(sent, myStemObj):\n",
    "    lemmat_list = []\n",
    "    try:\n",
    "        lemmas = myStemObj.lemmatize(sent)\n",
    "        for i in lemmas:\n",
    "            if i.isalpha():\n",
    "                lemmat_list.append(i)\n",
    "        return lemmat_list\n",
    "    except BrokenPipeError:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       [помнить]\n",
       "1                 [форрест, гамп]\n",
       "2                 [зеленый, миля]\n",
       "3                [крестный, отец]\n",
       "4                   [король, лев]\n",
       "                  ...            \n",
       "171               [мистер, сталь]\n",
       "172          [побег, невозможный]\n",
       "173         [милашка, в, розовый]\n",
       "174    [призрак, и, миссис, мьюр]\n",
       "175       [этот, старый, чувство]\n",
       "Name: filtered_film_names, Length: 176, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_films_1 = test_films.apply(lambda x: lemmatize_str(x, m))\n",
    "lemma_films_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['звездный', 'война', 'эпизод', 'возвращение', 'джедай']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_films_1[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "# w_tokenizer.tokenize(lemma_films_1[17])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def clean_tokens(text):\n",
    "#     regular = r'[\\*+\\#+\\№\\\"\\-+\\+\\=+\\?+\\&\\^\\.+\\;\\,+\\>+\\(\\)\\/+\\:\\\\+]'\n",
    "#     text = re.sub(regular, '', text)\n",
    "#     text = re.sub(r'\\s+', '', text)\n",
    "#     return text\n",
    "  \n",
    "# cleaned_text = []\n",
    "# tokens = []\n",
    " \n",
    "# # для каждого сообщения text из столбца data['Message']\n",
    "# for text in lemma_films_1.index:\n",
    "#     text = clean_tokens(text)\n",
    "#     cleaned_text.append(text)\n",
    "#     #разбиваем текст на токены с сохраняем результат в списке tokens\n",
    "#     text = word_tokenize(lemma_films_1[text])\n",
    "#     tokens.append(lemma_films_1[text])\n",
    "# film_tokens['cleaned_films'] = cleaned_text\n",
    "# # Сохраняем разбитые на токены сообщения в новой колонке 'Tokens_msg'\n",
    "# film_tokens['tokens_films'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sentence preproceccing task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
